[ { "title": null, "url": "/posts/2024-01-17-AIX-VIOS-security-advisories/", "categories": "", "tags": "", "date": "2024-01-17 15:48:38 +1100", "snippet": "IBM provide a number of resources to keep up to date with security advisory announcements. IBM X-Force Exchange provide an API, IBM Support provide email notifications (along with RSS/Atom feeds), ...", "content": "IBM provide a number of resources to keep up to date with security advisory announcements. IBM X-Force Exchange provide an API, IBM Support provide email notifications (along with RSS/Atom feeds), and there is also a JSON feed that comes from https://esupport.ibm.com/customercare/flrt/doc?page=aparJSON. For my requirements, I’ve found the JSON feed to contain the data that I need at a glance. It provides me with an abstract, the URL to the advisory, if the fix requires a reboot, and the CVE number along with its associated CVSS score.This is enough information for me to quickly triage a security advisory and determine if I need to look into it in any further detail. However, looking at raw JSON data isn’t pretty, and I wanted it presented in a format that was a little more involved than what I could do with jq. I ended up writing something in python that you can find on my GitHub page that produces the below formatted table.aix_security_advisories.py screenshotI’m not using all the available data in the output. Additional items include a link to download a fix (if available) and a list of impacted filesets and versions. It’s not data that I need at a quick glance, and it’s all available at the advisory URL. If anyone wants that data included in the table, I welcome contributions, but also happy to add it in. All this is just me trying to have a better understanding of working with python, and making my day-to-day easier." }, { "title": "HMC Profile Diff", "url": "/posts/HMC-profile-diff/", "categories": "hmc", "tags": "python, hmc", "date": "2023-01-16 00:00:00 +1100", "snippet": "More often than not, I find myself having to compare two Hardware Management Console (HMC) logical partition (LPAR) profile configurations. Sometimes this is to ensure that a profile in a disaster ...", "content": "More often than not, I find myself having to compare two Hardware Management Console (HMC) logical partition (LPAR) profile configurations. Sometimes this is to ensure that a profile in a disaster recovery site matches that of its production counterpart. Other times, it’s to make sure all members of a cluster have identical resource configurations.In a perfect world, I’d be manging the LPAR configurations using something like Terraform, but this currently isn’t an option. I can login to the HMC and manually verify the profiles (which is what I’ve been doing), but this is tedious and prone to error when you need to compare many profile pairs.The HMC, a few major version releases ago, introduced a REST API that exposes all the profile data. I’ve written a script to collect this data, and present it in a more user friendly format for consumption. You can find the HMC profile diff script hosted on my GitHub.Below are some screenshots of the script output.Compare all LPAR profile attributesCompare all LPAR profile attributes and only show differences" }, { "title": "AIX paging space", "url": "/posts/AIX-paging-space/", "categories": "aix", "tags": "aix, paging", "date": "2022-11-06 00:00:00 +1100", "snippet": "An AIX installation will come with a default configured paging space logical volume (hd6). You can increase the size of the logical volume to suit your needs, but at some stage, you may have a requ...", "content": "An AIX installation will come with a default configured paging space logical volume (hd6). You can increase the size of the logical volume to suit your needs, but at some stage, you may have a requirement to either expand the logical volume greater than the size of the rootvg, or more than 64GB (the current maximum size of a paging logical volume). There are a number of best practises when it comes to paging space on AIX. Don’t span paging logical volumes across different disks. Create additional paging logical volumes of equal size. As paging space is allocated in a round-robin type manner, smaller paging spaces will fill up quicker than larger ones. If you can’t increase the default paging logical volume hd6 to be equal size of all other additional paging logical volumes, deactivate it.As a rule of thumb, I generally try and keep my rootvg volume groups relatively lean. I’ve been bitten way too often with failed multibos creations due to insufficient free capacity in the rootvg volume group. More often than not, this is because someone has increased the size of the hd6 paging space. Depending on the size of your rootvg, you may wish to impose a maximum size limit on the hd6 paging logical volume. Any requirements for an increase to (or additional) paging space, should be created on a separate volume.If I have such a requirement for additional paging space (say the largest possible paging size of 64GB), I will create a separate volume group (for example, pagingvg) and allocate LUN’s of 64GB in size. Each paging logical volume will consume the entire LUN, and I will deactivate hd6 (see example below).# lsps -aPage Space Physical Volume Volume Group Size %Used Active Auto Type Chksumpaging01 hdisk2 pagingvg 61344MB 0 yes yes lv 0paging00 hdisk1 pagingvg 61344MB 0 yes yes lv 0hd6 hdisk0 rootvg 8192MB 0 no yes lv 0# lspv -l hdisk1hdisk1:LV NAME LPs PPs DISTRIBUTION MOUNT POINTpaging00 1917 1917 384..383..383..383..384 N/A# lspv -l hdisk2hdisk2:LV NAME LPs PPs DISTRIBUTION MOUNT POINTpaging01 1917 1917 384..383..383..383..384 N/AI then have the below script that runs at boot. It will deactivate the default paging device (hd6) if an alternate paging space is active, and greater in size.#!/bin/ksh## Script : deactivate_paging.sh## Description : Script runs at boot and will deactivate the default# AIX paging device (hd6) if an alternate paging space is# active, and greater in size.## Usage : Script takes no parameters.if lsps -ac | grep -Evq '^#|^hd6'; then # Get size of default paging space hd6_size=$(lsps -a | awk '/^hd6/{ print $4 }') # Create array of other paging space attributes set -A paging_name $(lsps -a | awk '!/^hd6/ &amp;&amp; (NR!=1){ print $1 }') set -A paging_size $(lsps -a | awk '!/^hd6/ &amp;&amp; (NR!=1){ print $4 }') set -A paging_active $(lsps -a | awk '!/^hd6/ &amp;&amp; (NR!=1){ print $6 }') # Default paging space (hd6) will be turned off if any single # alternate paging space is active, and greater in size than # the default paging space count=0 while [[ \"${count}\" -lt \"${#paging_name[*]}\" ]]; do if [[ ( \"${paging_active[$count]}\" = \"yes\" ) &amp;&amp; ( \"${paging_size[$count]%??}\" -gt \"${hd6_size%??}\" ) ]]; then { echo \"At least one alternate paging space detected and active [${paging_name[$count]}]\" echo 'Deactivating default paging space hd6...' swapoff /dev/hd6 } &gt; /dev/console exit else (( count+=1 )) fi donefiConsole output during system boot.# alog -t console -o......... 0 Mon Nov 7 10:23:14 AEDT 2022 At least one alternate paging space detected and active [paging00] 0 Mon Nov 7 10:23:14 AEDT 2022 Deactivating default paging space hd6...You can find some good information regarding paging space placement best practices on IBM’s website - https://www.ibm.com/support/pages/paging-space-placement-best-practices" }, { "title": "Deploying AIX eFixes using NIM at scale with Puppet", "url": "/posts/Deploying-AIX-eFixes-using-NIM-at-scale-with-Puppet/", "categories": "aix", "tags": "aix, nim, puppet", "date": "2022-09-10 00:00:00 +1000", "snippet": "Most will be familiar with using the emgr command to install emergency fixes (eFixes) on AIX. You can additionally use a Network Installation Management (NIM) server in either a push or pull operat...", "content": "Most will be familiar with using the emgr command to install emergency fixes (eFixes) on AIX. You can additionally use a Network Installation Management (NIM) server in either a push or pull operation to install fixes, which is handy when deploying at scale (more on this later in the post with deploying fixes at scale with Puppet).Prepare NIM resourcesThere is a little setup work that you first need to perform on the NIM server to make this possible. At a minimum, on the NIM server, you will require a lpp_source resource to place the fixes in. Creating a lpp_source resource is outside the scope of this post, but you can find additional information here.(nim01) # lsnim -l 7200TL5SP4_lpp7200TL5SP4_lpp: class = resources type = lpp_source comments = AIX 7.2 TL5 SP4 lppsource arch = power Rstate = ready for use prev_state = unavailable for use location = /nim/lpp/7200TL5SP4_lpp simages = yes alloc_count = 0 server = master(nim01) # ls -l /nim/lpp/7200TL5SP4_lpp/emgr/ppctotal 64320-rw-r--r-- 1 root system 28191901 Aug 16 00:49 1121201a.220804.epkg.Z-rw-r--r-- 1 root system 49935 Jun 11 03:37 IJ39876s3a.220506.epkg.Z-rw-r--r-- 1 root system 4683895 Aug 15 16:08 IJ41139m4a.220809.epkg.ZOptionally, if you’re installing multiple fixes at the same time, you can create a NIM installp_bundle resource. Creating an installp_bundle resource is outside the scope of this post, but you can find additional information here.(nim01) # lsnim -l 7200TL5SP4_emgr7200TL5SP4_emgr: class = resources type = installp_bundle comments = AIX 7.2 TL5 SP4 emgr_bundle Rstate = ready for use prev_state = unavailable for use location = /nim/bundles/7200TL5SP4_emgr alloc_count = 0 server = masterContents of /nim/bundles/7200TL5SP4_emgrE:IJ39876s3a.220506.epkg.ZE:IJ41139m4a.220809.epkg.ZE:1121201a.220804.epkg.ZInstalling fixes on AIX hostsYou have two options when it comes to installing the fixes onto AIX hosts using the NIM server. You can either push the fixes from the NIM server to the AIX host, or you can pull the fixes from the AIX host.PushInstalling a single fix from a NIM lpp_source resource.(nim01) # nim -o cust -a lpp_source=7200TL5SP4_lpp -a filesets=E:1121201a.220804.epkg.Z aix01Initializing log /var/adm/ras/emgr.log ...EPKG NUMBER LABEL OPERATION RESULT=========== ============== ================= ==============1 1121201a INSTALL SUCCESSReturn Status = SUCCESSInstalling multiple fixes from a NIM installp_bundle resource.(nim01) # nim -o cust -a lpp_source=7200TL5SP4_lpp -a installp_bundle=7200TL5SP4_emgr aix01Initializing log /var/adm/ras/emgr.log ...EPKG NUMBER LABEL OPERATION RESULT=========== ============== ================= ==============1 IJ39876s3a INSTALL SUCCESS2 IJ41139m4a INSTALL SUCCESS3 1121201a INSTALL SUCCESSReturn Status = SUCCESSPullInstalling a single fix from a NIM lpp_source resource.(aix01) # nimclient -o cust -a lpp_source=7200TL5SP4_lpp -a filesets=E:1121201a.220804.epkg.ZInitializing log /var/adm/ras/emgr.log ...EPKG NUMBER LABEL OPERATION RESULT=========== ============== ================= ==============1 1121201a INSTALL SUCCESSReturn Status = SUCCESSInstalling multiple fixes from a NIM installp_bundle resource.(aix01) # nimclient -o cust -a lpp_source=7200TL5SP4_lpp -a installp_bundle=7200TL5SP4_emgrInitializing log /var/adm/ras/emgr.log ...EPKG NUMBER LABEL OPERATION RESULT=========== ============== ================= ==============1 IJ39876s3a INSTALL SUCCESS2 IJ41139m4a INSTALL SUCCESS3 1121201a INSTALL SUCCESSReturn Status = SUCCESSDeploying fixes at scale with Puppet TasksIf you’re using Puppet Enterprise, and can use Puppet Tasks, here is one that allows you to install both a single fix, or multiple fixes using the above mentioned methods.Task metadata{ \"description\": \"A task for installing/uninstalling AIX fixes.\", \"parameters\": { \"install\": { \"description\": \"Set to true to install efix, or set to false to uninstall efix.\", \"type\": \"Boolean\" }, \"lpp_source\": { \"description\": \"NIM lpp_source containing the efix(s).\", \"type\": \"String[1]\" }, \"fix_name\": { \"description\": \"Name of a single efix to be installed/uninstalled.\\n\\tWhen installing, the 'fix_name' is the name of the package (e.g. IJ31191m1a.210322.epkg.Z).\\n\\tWhen uninstalling, the 'fix_name' is the name of the label (e.g. IJ31191m1a).\", \"type\": \"Optional[String[1]]\" }, \"installp_bundle\": { \"description\": \"Name of installp_bundle to use when installing multiple efixes.\", \"type\": \"Optional[String[1]]\" }, \"reboot\": { \"description\": \"If true, the system is rebooted after installing/uninstalling the efix(s).\", \"type\": \"Boolean\" } }, \"puppet_task_version\": 1, \"supports_noop\": false}Task#!/opt/puppetlabs/puppet/bin/ruby## A task for installing/uninstalling AIX fixesrequire 'puppet'require 'json'# Set FACTERLIB so custom facts resolvePuppet.initialize_settingsENV['FACTERLIB'] = Puppet.settings['factpath']require 'facter'# Sanity check that we're an AIX hostif Facter.value(:operatingsystem) == \"AIX\" then params = JSON.parse(STDIN.read) install = params['install'] lpp_source = params['lpp_source'] fix_name = params['fix_name'] installp_bundle = params['installp_bundle'] reboot = params['reboot'] if install then # Check if we have either a fix_name or installp_bundle set if params.key?(\"installp_bundle\") then fix = \"installp_bundle=#{installp_bundle}\" elsif params.key?(\"fix_name\") then fix = \"filesets=E:#{fix_name}\" else print \"You need to specify either a fix_name or installp_bundle.\" exit(2) end # Install efix(s) system(\"/usr/sbin/nimclient\", \"-o\", \"cust\", \"-a\", \"lpp_source=#{lpp_source}\", \"-a\", \"#{fix}\") if $?.exitstatus != 0 print \"efix failed to install.\" exit(2) end else # Uninstall efix if params.key?(\"fix_name\") then system(\"/usr/sbin/nimclient\", \"-o\", \"cust\", \"-a\", \"lpp_source=#{lpp_source}\", \"-a\", \"filesets=E:#{fix_name}\", \"-a\", \"installp_flags=u\") if $?.exitstatus != 0 print \"efix failed to uninstall.\" exit(2) end else print \"You need to specify a fix_name to uninstall.\" end end # Reboot if reboot then system(\"/usr/bin/echo '/usr/sbin/shutdown -r +3' | /usr/bin/at now\") endelse print \"Task only supported on AIX.\" exit(1)endRunning the Task$ puppet task show patch::aix_efixpatch::aix_efix - A task for installing/uninstalling AIX fixes.USAGE:$ puppet task run patch::aix_efix install=&lt;value&gt; lpp_source=&lt;value&gt; reboot=&lt;value&gt; [fix_name=&lt;value&gt;] [installp_bundle=&lt;value&gt;] &lt;[--nodes, -n &lt;node-names&gt;] | [--query, -q &lt;'query'&gt;]&gt;PARAMETERS:- install : Boolean Set to true to install efix, or set to false to uninstall efix.- lpp_source : String[1] NIM lpp_source containing the efix(s).- reboot : Boolean If true, the system is rebooted after installing/uninstalling the efix(s).- fix_name : Optional[String[1]] Name of a single efix to be installed/uninstalled. When installing, the 'fix_name' is the name of the package (e.g. IJ31191m1a.210322.epkg.Z). When uninstalling, the 'fix_name' is the name of the label (e.g. IJ31191m1a).- installp_bundle : Optional[String[1]] Name of installp_bundle to use when installing multiple efixes.$ puppet task run patch::aix_efix install=true lpp_source=7200TL5SP4_lpp reboot=false installp_bundle=7200TL5SP4_emgr -n aix01Starting job ...Note: The task will run only on permitted nodes.New job ID: 83786Nodes: 1Started on aix01 ...Finished on node aix01 STDOUT: Initializing log /var/adm/ras/emgr.log ... EPKG NUMBER LABEL OPERATION RESULT =========== ============== ================= ============== 1 IJ39876s3a INSTALL SUCCESS 2 IJ41139m4a INSTALL SUCCESS 3 1121201a INSTALL SUCCESS Return Status = SUCCESSJob completed. 1/1 nodes succeeded.Duration: 1 min, 3 sec" }, { "title": "Display client LPAR WWPNs from the HMC", "url": "/posts/LPAR-WWPNs-from-the-HMC/", "categories": "hmc", "tags": "hmc, scripting, shell, shell-scripting, storage", "date": "2022-08-13 00:00:00 +1000", "snippet": "There are several ways to find the WWPNs for a virtual fibre channel adapter. You can log into the individual LPAR’s, get them from the Hardware Management Console (HMC), or from another asset disc...", "content": "There are several ways to find the WWPNs for a virtual fibre channel adapter. You can log into the individual LPAR’s, get them from the Hardware Management Console (HMC), or from another asset discovery tool that may contain this data.Using EZH functions originally written by Brian Smith, I’ve added a function named lparwwpn that takes a list of LPAR names as arguments, and prints the virtual fibre channel WWPNs.kristijan@hmc:~&gt; source lparwwpnkristijan@hmc:~&gt; lparwwpn aix1 aix2aix1 [POWER-FRAME-1]c0XXXXXXXXXXXXf2,c0XXXXXXXXXXXXf3c0XXXXXXXXXXXXf4,c0XXXXXXXXXXXXf5c0XXXXXXXXXXXXf6,c0XXXXXXXXXXXXf7c0XXXXXXXXXXXXf8,c0XXXXXXXXXXXXf9aix2 [POWER-FRAME-2]c0XXXXXXXXXXXX20,c0XXXXXXXXXXXX21c0XXXXXXXXXXXX22,c0XXXXXXXXXXXX23c0XXXXXXXXXXXX24,c0XXXXXXXXXXXX25c0XXXXXXXXXXXX26,c0XXXXXXXXXXXX27Save the contents of the shell script below to a file named lparwwpn, and copy it over to the HMC. You can name the file anything you like, just adjust the source command above accordingly.lparframelookup () { while read -r system; do while read -r lpar; do if [ \"$lpar\" = \"$*\" ]; then echo \"${system}\"; fi done &lt; &lt;(lssyscfg -m \"$system\" -r lpar -F name) done &lt; &lt;(lssyscfg -r sys -F \"name,state\" | grep -E \"Standby|Operating\" | cut -d, -f 1) | tail -n 1}checklpar () { count=0; if [ -n \"$1\" ]; then while read -r system; do while read -r l; do if [ \"$l\" = \"$1\" ] ; then count=$((count +1)) fi done &lt; &lt;(lssyscfg -m \"$system\" -r lpar -F name) done &lt; &lt;(lssyscfg -r sys -F \"name,state\" | grep -E \"Standby|Operating\" | cut -d, -f 1) l=\"\" if [ $count -eq 0 ]; then echo \"ERROR: LPAR not found: $1\" return 1 fi if [ $count -gt 1 ]; then echo \"ERROR: Multiple LPAR's with same name $1\" return 2 fi else unset input unset lpararray echo \"Select LPAR: \" while read -r system; do while read -r l; do count=$((count +1)) printf \"%5s. %-20s %-20s\\n\" $count \"$l\" \"$(lssyscfg -r lpar -m \\\"\"${system}\"\\\" -F state --filter lpar_names=\\\"\"${l}\"\\\")\" lpararray[$count]=\"$l\" done &lt; &lt;(lssyscfg -m \"$system\" -r lpar -F name) done &lt; &lt;(lssyscfg -r sys -F \"name,state\" | grep -E \"Standby|Operating\" | cut -d, -f 1) echo while [ -z \"${lpararray[$input]}\" ]; do printf \"%s\\n\" \"Enter LPAR number (1-$count, \\\"q\\\" to quit): \"; read -r input if [ \"$input\" = \"q\" -o \"$input\" = \"Q\" ]; then return 1; fi done lpar=\"${lpararray[$input]}\" checklpar \"$lpar\" || return 2 fi}lparwwpn () { for lpar in \"$@\"; do frame=$(lparframelookup \"${lpar}\") printf \"\\n%s [%s]\\n\" \"${lpar}\" \"${frame}\" checklpar \"$lpar\" &amp;&amp; eval lshwres -m \"${frame}\" -r virtualio --rsubtype fc --level lpar --filter \\\"lpar_names=\"${lpar}\"\\\" | cut -d '=' -f11 | sed 's/\\\"//' | sort; done}I’ve recently come across EEZH, which appears to be a fork of Brian Smith’s original EZH code. Might be more beneficial to add lparwwpn into that project.While we’re on the subject of displaying WWPNs, here is a recent post from Chris Gibson on Displaying WWPN information on Power10 Systems." }, { "title": "AIX mksysb creation date", "url": "/posts/AIX-mksysb-creation/", "categories": "aix", "tags": "python, scripting, shell, shell-scripting", "date": "2022-07-24 00:00:00 +1000", "snippet": "Something that I do quite frequently is verify that I have a valid mksysb image for the AIX LPAR that I’m about to make changes on (for example, before applying a service pack). You can do this fro...", "content": "Something that I do quite frequently is verify that I have a valid mksysb image for the AIX LPAR that I’m about to make changes on (for example, before applying a service pack). You can do this from the AIX LPAR itself using the nimclient command.kristijan@aix13 # nimclient -l -L -t mksysb aix13 | grep aix13aix13_220720_img mksysbaix13_220713_img mksysbkristijan@aix13 # nimclient -l -l aix13_220720_imgaix13_220720_img: class = resources type = mksysb creation_date = Wed Jul 20 00:17:25 2022 source_image = aix13 arch = power Rstate = ready for use prev_state = unavailable for use location = /mksysb/aix13/aix13_220720_img version = 7 release = 2 mod = 5 oslevel_r = 7200-05 oslevel_s = 7200-05-04-2220 alloc_count = 0 server = masterIn my AIX environment, I define a “valid” mksysb as one that was created within the last 2 weeks. Anything created after that date I’d be hesitant in restoring from. Parsing the creation_date with a shell script isn’t all that elegant. The python datetime module allows us to parse this easily.The script does assume (on line 18) that your NIM mksysb resource names contain the hostname you’re running the script from.#!/usr/bin/env python3## Check that there is at least one mksysb for the client, and# that the creation date of the mksysb is not older than 15 daysimport sysimport socketimport subprocessfrom datetime import datetime# Get hostnamehostname = socket.gethostname()# Get current timecurrent_time = datetime.now()# Create list of mksysb resources from the NIM servernim_mksysb_list = subprocess.check_output(f\"/usr/sbin/nimclient -l -L -t mksysb {hostname} | /usr/bin/awk '/{hostname}/{{ print $1 }}'\", shell=True, encoding='utf-8').split()# If the subprocess above returns no values, the result is a# single item list with an empty string. Let's strip that out.nim_mksysb_list = filter(None, nim_mksysb_list)# Parse list of NIM mksysb backups and compare creation dateif nim_mksysb_list: for mksysb in nim_mksysb_list: mksysb_creation_time = subprocess.check_output(f\"/usr/sbin/nimclient -l -l {mksysb} | /usr/bin/awk -F = '/creation_date/{{ print $2 }}'\", shell=True, encoding='utf-8').strip() mksysb_creation_time = datetime.strptime(''.join(mksysb_creation_time), '%c') elapsed = current_time - mksysb_creation_time if elapsed.days &lt; 15: # mksysb found on NIM and is not older than 15 days sys.exit(0)else: # List is empty, no mksysb backups on NIM. sys.exit(1)# If we've made it this far, all mksysb's are older than 15 dayssys.exit(2)The above script will return the following exit codes: Exit code Description 0 mksysb found and is not older than 15 days 1 no mksysb found 2 all mksysbs found are older than 15 days The use of exit codes to determine the validity of a mksysb is because its intended use is part of a larger pipeline of checks that occur before an AIX LPAR is patched. It forms one of many pre-checks that are run on an AIX LPAR before it’s deemed healthy to be patched." }, { "title": "Migrating from SDDPCM to AIXPCM (the easy way)", "url": "/posts/Migrating-from-SDDPCM-to-AIXPCM/", "categories": "aix", "tags": "aix, aixpcm, sddpcm, storage", "date": "2018-07-30 00:00:00 +1000", "snippet": "For a while now, IBM have diverted their efforts in storage multipathing from SDDPCM to the default AIX PCM. This brings a few advantages, but specifically for me, it means the driver is now update...", "content": "For a while now, IBM have diverted their efforts in storage multipathing from SDDPCM to the default AIX PCM. This brings a few advantages, but specifically for me, it means the driver is now updated as part of the AIX system maintenance, and is no longer something I need to maintain separately. All significant functionality that SDDPCM provides, can now be provided by the default AIX PCM driver. Additionally, SDDPCM is not supported on POWER 9 hardware.For those currently using SDDPCM, removing the driver can be somewhat complicated, and even more so when the boot LUN (rootvg LUN) is being managed by SDDPCM. Buried deep in the Multipath Subsystem Device Driver Users Guide is a command called manage_disk_drivers. The manage_disk_drivers command can be used to display a list of storage families, and the driver that manages or supports each family. The command also allows us to easily switch (with a reboot if you boot from an SDDPCM managed device) the driver from SDDPCM to AIXPCM (or vice versa). Below I detail how to switch from SDDPCM to AIXPCM when using LUN’s presented to the host via SVC.Existing driver manging IBMSVCFrom the man page for manage_disk_drivers If the present driver attribute is set to NO_OVERRIDE, the AIX operating system selects an alternate path control module (PCM), such as Subsystem Device Driver Path Control Module (SDDPCM), if it is installed.# manage_disk_drivers -lDevice Present Driver Driver Options2810XIV AIX_AAPCM AIX_AAPCM,AIX_non_MPIODS4100 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4200 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4300 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4500 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4700 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4800 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS3950 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS5020 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDCS3700 AIX_APPCM AIX_APPCMDCS3860 AIX_APPCM AIX_APPCMDS5100/DS5300 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS3500 AIX_APPCM AIX_APPCMXIVCTRL MPIO_XIVCTRL MPIO_XIVCTRL,nonMPIO_XIVCTRL2107DS8K NO_OVERRIDE NO_OVERRIDE,AIX_AAPCM,AIX_non_MPIOIBMFlash NO_OVERRIDE NO_OVERRIDE,AIX_AAPCM,AIX_non_MPIOIBMSVC NO_OVERRIDE NO_OVERRIDE,AIX_AAPCM,AIX_non_MPIOSwitch to using AIXPCM# manage_disk_drivers -d IBMSVC -o AIX_AAPCM ********************** ATTENTION ************************* For the change to take effect the system must be rebootedAfter the reboot, you will now see AIX_AAPCM as the present driver being used.# manage_disk_drivers -lDevice Present Driver Driver Options2810XIV AIX_AAPCM AIX_AAPCM,AIX_non_MPIODS4100 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4200 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4300 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4500 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4700 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS4800 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS3950 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS5020 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDCS3700 AIX_APPCM AIX_APPCMDCS3860 AIX_APPCM AIX_APPCMDS5100/DS5300 AIX_SDDAPPCM AIX_APPCM,AIX_SDDAPPCMDS3500 AIX_APPCM AIX_APPCMXIVCTRL MPIO_XIVCTRL MPIO_XIVCTRL,nonMPIO_XIVCTRL2107DS8K NO_OVERRIDE NO_OVERRIDE,AIX_AAPCM,AIX_non_MPIOIBMFlash NO_OVERRIDE NO_OVERRIDE,AIX_AAPCM,AIX_non_MPIOIBMSVC AIX_AAPCM NO_OVERRIDE,AIX_AAPCM,AIX_non_MPIOFrom here, you can do one of two things. Leave the SDDPCM driver installed, as this will allow for easy rollback should you experience performance issues, or other driver related problems. Or completely remove the SDDPCM driver from the LPAR.A few things to keep in mind. If you’ve modified the queue_depth attribute on the hdisk, this will be reset to the AIXPCM default of 20. There is already a good write up on best practises and considerations when working with the default AIXPCM driver. When using SDDPCM, you would have used the pcmpath command to display and manage devices. As above, someone has already written a good write up on resiliency and problem determination, and some common lsmpio commands you’ll want to know.Taking the IBM recommendations into account, I’ll set the hdisk attributes accordingly.# lsdev -Cc disk -F name | while read -r hdisk; do chdev -l ${hdisk} -a queue_depth=32 -a reserve_policy=no_reserve -a algorithm=shortest_queue -P; donehdisk0 changedhdisk1 changedAnother handy command, which isn’t related to the overall driver migration, is using chdef to change the default values of the predefined attributes in ODM. Any future LUN’s presented to the host will now have the queue_depth, reserve_policy, and algorithm set to the values I want.# chdef -a queue_depth=32 -c disk -s fcp -t mpioosdiskqueue_depth changed# chdef -a reserve_policy=no_reserve -c disk -s fcp -t mpioosdiskreserve_policy changed# chdef -a algorithm=shortest_queue -c disk -s fcp -t mpioosdiskalgorithm changedRollbackShould you need to go back to using SDDPCM as the driver, and haven’t removed it, you can use manage_disk_drivers to flip back and reboot.# manage_disk_drivers -d IBMSVC -o NO_OVERRIDE ********************** ATTENTION ************************* For the change to take effect the system must be rebooted" }, { "title": "HMC Elastic CoD detail function", "url": "/posts/HMC-Elastic-CoD-detail-function/", "categories": "hmc", "tags": "cod, elastic-capacity-on-demand, hmc, scripting, shell, shell-scripting", "date": "2015-11-19 00:00:00 +1100", "snippet": "IBM have created a self-service portal for its customers to allow them to request their own Elastic Capacity on Demand codes for their registered systems. In my time using the new website, the code...", "content": "IBM have created a self-service portal for its customers to allow them to request their own Elastic Capacity on Demand codes for their registered systems. In my time using the new website, the codes have been generated and sent to me via email on average between 30 &amp; 45 minutes. This significantly reduces not only the time taken to get new codes posted to the POD website, but also eliminates the process of having to reach out to your IBM representative to request them from the COD office in the USA. The website does require a number of fields to be filled in for the code to be generated automatically: System type System serial number Anchor card CCIN Anchor card serial number Anchor card unique identifier Resource identifier Activate resources Sequence number Entry checkAll this information can be gathered from either the HMC GUI or the CLI. Preferring to work on the CLI, I’ve written a function (inspiration from Brian Smith) that collects and generates all the data the website requires in a nice to read format (well, nicer to read than what’s presented via the lscod command).kristijan@hmc:~&gt; source onoffdetails kristijan@hmc:~&gt; onoffdetailsUsage: onoffdetails [managed system] [mem | proc] kristijan@hmc:~&gt; onoffdetails 9119-FHB-SN1234567 mem Memory details for 9119-FHB-SN1234567 System type : 9119 System serial number : 12-34567 Anchor card CCIN : 52C4 Anchor card serial number : 00-236D000 Anchor card unique identifier : 316405567831037C Resource identifier : D973 Activate resources : 0540 Sequence number : 0046 Entry check : A3 kristijan@hmc:~&gt; onoffdetails 9119-FHB-SN1234567 proc Processor details for 9119-FHB-SN1234567 System type : 9119 System serial number : 12-34567 Anchor card CCIN : 52C4 Anchor card serial number : 00-236D000 Anchor card unique identifier : 316405567831037C Resource identifier : D971 Activate resources : 0450 Sequence number : 0045 Entry check : A1Function code is below, you just need to copy it over to the HMC, and then source it after you’ve logged in.# Script: onoffdetails## Usage: Gathers all the details required to self request On/Off# Capacity on Demand from IBM's self-service portal.# (https://www-304.ibm.com/support/customercare/ss/escod/home)## Change log:# Date Who Comment# ---- -- -------# 18/11/15 Kristian Milos Initial write.#onoffdetails () {if [ $# -lt 2 ]; then printf \"Usage: onoffdetails [managed system] [mem | proc]\\n\"else COUNT=0 ERROR=0 while read SYSTEM; do if [ \"${SYSTEM}\" = \"$1\" ] ; then COUNT=$((COUNT +1)) fi done &lt; &lt;(lssyscfg -r sys -F \"name\") if [ ${COUNT} -eq 0 ]; then printf \"\\nERROR: System not found: $1\\n\\n\" printf \"Known managed systems:\\n\" lssyscfg -r sys -F name ERROR=1 fi if [[ $2 = \"mem\" || $2 = \"proc\" &amp;&amp; ${ERROR} != 1 ]]; then SYSTEM=$1 RESOURCE=$2 sys_type=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F sys_type) sys_serial_num=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F sys_serial_num) anchor_card_ccin=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F anchor_card_ccin) anchor_card_serial_num=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F anchor_card_serial_num) anchor_card_unique_id=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F anchor_card_unique_id) resource_id=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F resource_id) activated_resources=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F activated_resources) sequence_num=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F sequence_num) entry_check=$(lscod -m ${SYSTEM} -t code -r ${RESOURCE} -c onoff -F entry_check) if [ ${RESOURCE} == \"mem\" ]; then printf \"\\nMemory details for ${SYSTEM}\\n\\n\" elif [ ${RESOURCE} == \"proc\" ]; then printf \"\\nProcessor details for ${SYSTEM}\\n\\n\" fi printf \" System type : ${sys_type}\\n\" printf \" System serial number : ${sys_serial_num}\\n\" printf \" Anchor card CCIN : ${anchor_card_ccin}\\n\" printf \" Anchor card serial number : ${anchor_card_serial_num}\\n\" printf \" Anchor card unique identifier : ${anchor_card_unique_id}\\n\" printf \" Resource identifier : ${resource_id}\\n\" printf \" Activate resources : ${activated_resources}\\n\" printf \" Sequence number : ${sequence_num}\\n\" printf \" Entry check : ${entry_check}\\n\" else printf \"\\nUsage: onoffdetails [managed system] [mem | proc]\\n\" fifi}" }, { "title": "PowerHA SystemMirror and NIM mksysb failures", "url": "/posts/PowerHA-SystemMirror-and-NIM-mksysb-failures/", "categories": "aix, hacmp, powerha", "tags": "aix, hacmp, mksysb, nim, powerha, systemmirror", "date": "2015-02-18 00:00:00 +1100", "snippet": "I built a basic two-node PowerHA SystemMirror (HACMP) cluster for my team a little while ago to use as a test environment for patch updates. While it wasn’t a true reflection of how the production ...", "content": "I built a basic two-node PowerHA SystemMirror (HACMP) cluster for my team a little while ago to use as a test environment for patch updates. While it wasn’t a true reflection of how the production environment is configured, it was enough to test functionality. As such, I configured a single virtual ethernet adapter in each cluster node, which would house both the boot IP and the service IP of the cluster. After a couple of weeks, I noticed that my weekly NIM mksysb’s on one of the two cluster nodes was always failing. Further investigation found that the NIM mksysb’s would always fail on the cluster node that had the active resource group with the service IP attached to it. If I failed the resource group over to the other cluster node, the NIM mksysb would complete successfully. The IP addresses in the environment (IP addresses changed/masked from actuals). NIM (hostname: nim) Cluster boot IP (hostname: powerha1) Service IP (hostname: powerha1-vip) 10.10.10.100 10.10.10.101 10.10.10.102 The error from the NIM log.NIM# nim -o define -t mksysb -a server=master -a location=/export/nim/mksysb/powerha1.mksysb -a source=powerha1 -a mk_image=yes -a mksysb_flags=iepmX powerha1_mksysb0042-001 nim: processing error encountered on \"master\": 0042-001 m_mkbosi: processing error encountered on \"powerha1\": warning: 0042-175 c_mkbosi: An unexpected result was returned by the \"/usr/sbin/mount\" command: mount: 1831-011 access denied for nim:/export/nim/mksysbmount: 1831-008 giving up on:nim:/export/nim/mksysbThe file access permissions do not allow the specified action.rc=1750042-175 c_mkbosi: An unexpected result was returned by the \"/usr/sbin/mount\" command:mount: 1831-011 access denied for nim:/export/nim/mksysbmount: 1831-008 giving up on:nim:/export/nim/mksysbThe file access permissions do not allow the specified action.Interface configuration on powerha1.powerha1# ifconfig -aen0: flags=1e084863,104c0&lt;UP,BROADCAST,NOTRAILERS,RUNNING,SIMPLEX,MULTICAST,GROUPRT,64BIT,CHECKSUM_OFFLOAD(ACTIVE),LARGESEND,CHAIN&gt; inet 10.10.10.102 netmask 0xffffff00 broadcast 10.10.10.255 inet 10.10.10.101 netmask 0xffffff00 broadcast 10.10.10.255 tcp_sendspace 262144 tcp_recvspace 262144 rfc1323 1lo0: flags=e08084b,c0&lt;UP,BROADCAST,LOOPBACK,RUNNING,SIMPLEX,MULTICAST,GROUPRT,64BIT,LARGESEND,CHAIN&gt; inet 127.0.0.1 netmask 0xff000000 broadcast 127.255.255.255 tcp_sendspace 131072 tcp_recvspace 131072 rfc1323 1powerha1# traceroute nimtrying to get source for nimsource should be 10.10.10.102traceroute to nim (10.10.10.100) from 10.10.10.102 (10.10.10.102), 30 hops maxoutgoing MTU = 1500 1 10.10.10.252 (10.10.10.252) 2 ms 1 ms 2 ms 2 nim (10.10.10.100) 1 ms 1 ms 1 mspowerha1# netstat -inName Mtu Network Address Ipkts Ierrs Opkts Oerrs Collen0 1500 link#2 1a.d8.5f.87.26.15 81979866 0 34796223 0 0en0 1500 10.10.10 10.10.10.102 81979866 0 34796223 0 0en0 1500 10.10.10 10.10.10.101 81979866 0 34796223 0 0lo0 16896 link#1 2421883 0 2421883 0 0lo0 16896 127 127.0.0.1 2421883 0 2421883 0 0As you can see from the traceroute and netstat above, because the Service IP (10.10.10.102) of the resource group is configured as the first alias on the en0 interface, the NFS mount request back to the NIM server occurs from this source IP address. However, back on the NIM server, the NIM client definition has the source address as 10.10.10.101. This is why the NIM server fails to mount the NFS mount on the client to take a successful mksysb.Fortunately, there is an option called Distribution Preference that can be configured on the Service IP. For this particular configuration, that has both the Service IP and Boot IP on the same interface, we want to Disable Firstalias. For the change to reflect across the cluster nodes, we need to Verify and Synchronize Cluster Configuration, and then bring that particular resource group offline, and the back online.You can find the option using the following fastpath: smitty cm_service_ip, or drilling down the following menu options.Cluster Applications and Resources &gt; Resources &gt; Configure Service IP Labels/Addresses &gt; Configure Service IP Labels/Address Distribution PreferenceAfter making the change, the order of the IP aliases within the powerha1 node look like this.powerha1# ifconfig -aen0: flags=1e084863,104c0&lt;UP,BROADCAST,NOTRAILERS,RUNNING,SIMPLEX,MULTICAST,GROUPRT,64BIT,CHECKSUM_OFFLOAD(ACTIVE),LARGESEND,CHAIN&gt; inet 10.10.10.101 netmask 0xffffff00 broadcast 10.10.10.255 inet 10.10.10.102 netmask 0xffffff00 broadcast 10.10.10.255 tcp_sendspace 262144 tcp_recvspace 262144 rfc1323 1lo0: flags=e08084b,c0&lt;UP,BROADCAST,LOOPBACK,RUNNING,SIMPLEX,MULTICAST,GROUPRT,64BIT,LARGESEND,CHAIN&gt; inet 127.0.0.1 netmask 0xff000000 broadcast 127.255.255.255 tcp_sendspace 131072 tcp_recvspace 131072 rfc1323 1powerha1# traceroute nimtrying to get source for nimsource should be 10.10.10.101traceroute to nim (10.10.10.100) from 10.10.10.101 (10.10.10.101), 30 hops maxoutgoing MTU = 1500 1 10.10.10.252 (10.10.10.252) 2 ms 1 ms 2 ms 2 nim (10.10.10.100) 1 ms 1 ms 1 mspowerha1# netstat -inName Mtu Network Address Ipkts Ierrs Opkts Oerrs Collen0 1500 link#2 1a.d8.5f.87.26.15 82006462 0 34814141 0 0en0 1500 10.10.10 10.10.10.101 82006462 0 34814141 0 0en0 1500 10.10.10 10.10.10.102 82006462 0 34814141 0 0lo0 16896 link#1 2430058 0 2430058 0 0lo0 16896 127 127.0.0.1 2430058 0 2430058 0 0After this change, both PowerHA SystemMirror cluster nodes successfully take NIM mksysb backups. Thanks goes out to Chris Gibson @cgibbo in helping resolve this issue." }, { "title": "Patching CVE-2014-6271 and CVE-2014-7169 on AIX via NIM (bash bug aka shellshock)", "url": "/posts/Patching-CVE-2014-6271-and-CVE-2014-7169-on-AIX-via-NIM/", "categories": "aix", "tags": "aix, nim, scripting, shell, shell-scripting", "date": "2014-09-30 00:00:00 +1000", "snippet": "Below I detail how I patched over 800 AIX LPAR’s that were exposed by CVE-2014-6271 and CVE-2014-7169, also known as shellshock, using the NIM server.From everything that I’ve been reading on IBM’s...", "content": "Below I detail how I patched over 800 AIX LPAR’s that were exposed by CVE-2014-6271 and CVE-2014-7169, also known as shellshock, using the NIM server.From everything that I’ve been reading on IBM’s Knowledge Centre, creating an LPP source containing only RPM’s isn’t possible. To patch my AIX environment, I decided to use the script resource available to the NIM master, along with the pre-existing NFS mounts that I had configured. Configure NFS share. NIM:kristian# cat /etc/exports /export/nim/images -ro,anon=0 NIM:kristian# showmount -e export list for NIM: /export/nim/images (everyone) Download patched bash RPM to NIM master. NIM:kristian# ls -l /export/nim/images/bash_CVE-2014-6271-7169 total 3448 -rw-r----- 1 root system 1765643 Sep 30 08:22 bash-4.2-17.aix5.1.ppc.rpm Script to patch bash /export/nim/patches/install_bash_CVE-2014-6271-7169 #!/bin/ksh # # Script to install new version of bash to # patch CVE-2014-6271 and CVE-2014-7169 # # Kristian Milos (29/09/14) # Get NIM master hostname NIM_MASTER_HOSTNAME=`grep NIM_MASTER_HOSTNAME /etc/niminfo | awk -F = '{ print $2 }'` # Create temporary mount location mkdir /install_bash_CVE-2014-6271-7169 # NFS mount patch mount ${NIM_MASTER_HOSTNAME}:/export/nim/images/bash_CVE-2014-6271-7169 /install_bash_CVE-2014-6271-7169 # Install patch rpm -Uvh /install_bash_CVE-2014-6271-7169/bash-4.2-17.aix5.1.ppc.rpm # Unmount NFS mount umount /install_bash_CVE-2014-6271-7169 # Remove temporary mount location rm -r /install_bash_CVE-2014-6271-7169 exit Define NIM resource Now that we have the location of the RPM on the NIM master, and the script that will be run on the NIM client to patch bash, we can now define a NIM script resource. NIM:kristian# nim -o define -t script \\ -a server=master \\ -a location=/export/nim/patches/install_bash_CVE-2014-6271-7169 \\ -a comments=\"bash fix for CVE-2014-6271 and CVE-2014-7169\" bash_CVE-2014-6271-7169 NIM:kristian# lsnim -l bash_CVE-2014-6271-7169 bash_CVE-2014-6271-7169: class = resources type = script comments = bash fix for CVE-2014-6271 and CVE-2014-7169 Rstate = ready for use prev_state = unavailable for use location = /export/nim/patches/install_bash_CVE-2014-6271-7169 alloc_count = 0 server = master Define NIM machine group We will now create a NIM machine group that will contain all the NIM clients that we will update. I find the easiest way to do this is by listing out all the NIM client definitions in the format required for the group define command. An example is shown below. NIM:kristian# for i in `lsnim -t standalone | awk '{ print $1 }'`; do echo \"-a add_member=$i \\\\\"; done -a add_member=aix1 \\ -a add_member=aix2 \\ -a add_member=aix3 \\ -a add_member=aix4 \\ -a add_member=aix5 \\ -a add_member=aix6 \\ Define the NIM group NIM:kristian# nim -o define -t mac_group \\ -a add_member=aix1 \\ -a add_member=aix2 \\ -a add_member=aix3 \\ -a add_member=aix4 \\ -a add_member=aix5 \\ -a add_member=aix6 PROD_LPARS Validate NIM master to client communications The next thing I do is validate that the NIM master can actually talk to all the NIM clients in the machine group. NIM:kristian# for srv in `lsnim -g PROD_LPARS | grep member | grep -v EXCLUDED | awk '{ print $3 }' | sed 's/;//' | sort`; do printf \"%-20s\" $srv; nim -o lslpp $srv &gt;/dev/null 2&gt;&amp;1; [ \"$?\" == 0 ] &amp;&amp; echo OK || echo \"Problem\"; done aix1 OK aix2 OK aix3 OK aix4 OK aix5 Problem aix6 OK Exclude uncontactable hosts For any NIM client that returns “Problem”, I exclude them from the NIM group operation NIM:kristian# nim -o select -a exclude=aix5 PROD_LPARS Patch all NIM clients via NIM master We’re now in a position to execute the patch across all the NIM clients listed in the group definition. NIM:kristian# nim -o cust -a script=bash_CVE-2014-6271-7169 -a concurrent=10 PROD_LPARS +-----------------------------------------------------------------------------+ Concurrency Control +-----------------------------------------------------------------------------+ Processing will begin with the first 5 machines from the group... +-----------------------------------------------------------------------------+ Initiating \"cust\" Operation +-----------------------------------------------------------------------------+ Allocating resources ... Initiating the cust operation on machine 1 of 5: aix1 ... Initiating the cust operation on machine 2 of 5: aix2 ... Initiating the cust operation on machine 3 of 5: aix3 ... Initiating the cust operation on machine 4 of 5: aix4 ... Initiating the cust operation on machine 5 of 5: aix6 ... +-----------------------------------------------------------------------------+ \"cust\" Operation Summary +-----------------------------------------------------------------------------+ Target Result ------ ------ aix1 INITIATED aix2 INITIATED aix3 INITIATED aix4 INITIATED aix6 INITIATED Note: Use the lsnim command to monitor progress of \"INITIATED\" targets by viewing their NIM database definition. +-----------------------------------------------------------------------------+ Concurrency Control +-----------------------------------------------------------------------------+ The first 8 machines have been processed. As machines finish installing processing will resume with the remaining members of the group, one at a time. +-----------------------------------------------------------------------------+ Concurrency Control: \"cust\" Operation Summary +-----------------------------------------------------------------------------+ Target Result ------ ------ aix1 COMPLETE aix2 COMPLETE aix3 COMPLETE aix4 COMPLETE aix6 COMPLETE Validate installed version of bash Once the process has completed, you can validate the version of bash installed across all NIM clients by running the following command. NIM:kristian# for i in `lsnim -g PROD_LPARS | grep member | grep -v EXCLUDED | awk '{ print $3 }' | sed 's/;//' | sort`; do echo $i; nim -o lslpp -a lslpp_flags=-Lc -a filesets=bash $i | grep bash | awk -F : '{ print $2 }'; echo \"\"; done aix1 bash-4.2-17 aix2 bash-4.2-17 aix3 bash-4.2-17 aix4 bash-4.2-17 aix6 bash-4.2-17 Include previously excluded NIM members One final clean up task, is to ensure you include all NIM members back into the group if you excluded them previously. NIM:kristian# nim -o select -a include_all=yes PROD_LPARS " }, { "title": "AIX boot hangs with HMC 2700 LED code", "url": "/posts/AIX-boot-hangs-with-HMC-2700-LED-code/", "categories": "aix", "tags": "2700, aix, boot, hang, hmc", "date": "2013-08-24 00:00:00 +1000", "snippet": "We recently upgraded the firmare on our Power frame, which required shutting down some of our AIX LPAR’s. The firmware upgrade went well, as did starting up all the AIX LPAR’s, except for one. This...", "content": "We recently upgraded the firmare on our Power frame, which required shutting down some of our AIX LPAR’s. The firmware upgrade went well, as did starting up all the AIX LPAR’s, except for one. This particular LPAR booted to HMC LED code 2700 and hung there. I restarted the partition to the Open Firmware (OF) prompt, and tried booting again using verbose mode to see where the boot process was hanging.----------------Attempting to configure device 'fscsi6' cfgmgr Time: 0 LEDS: 0x2700Invoking /usr/lib/methods/cfgefscsi -1 -l fscsi6Number of running methods: 4----------------Completed method for: fcs7, Elapsed time = 0Return code = 0***** stdout *****fscsi7*** no stderr ****----------------Time: 0 LEDS: 0x2700 for fscsi6Number of running methods: 3 cfgmgr exec(/bin/sh,-c,/usr/lib/methods/cfgefscsi -1 -l fscsi6){135238,102466}----------------Attempting to configure device 'fscsi7' cfgmgr Time: 0 LEDS: 0x2700Invoking /usr/lib/methods/cfgefscsi -1 -l fscsi7exec(/usr/lib/methods/cfgefscsi,-1,-l,fscsi6){135238,102466}Number of running methods: 4exec(/bin/sh,-c,/usr/lib/methods/cfgefscsi -1 -l fscsi7){122944,102466}exec(/usr/lib/methods/cfgefscsi,-1,-l,fscsi7){122944,102466}----------------Completed method for: fscsi7, Elapsed time = 0Return code = 0*** no stdout ******* no stderr ****----------------Time: 0 LEDS: 0x2700 for fscsi6Number of running methods: 3 cfgmgr ----------------Completed method for: fscsi5, Elapsed time = 0Return code = 0*** no stdout ******* no stderr ****----------------Time: 0 LEDS: 0x2700 for fscsi6Number of running methods: 2 cfgmgrTo verbose boot from OF, you can do the following from the HMC.$ chsysstate -r lpar -m &lt;frame&gt; -o on -n -b of -f -p &lt;lpar&gt;$ mkvterm -m &lt;frame&gt; -p &lt;lpar&gt;0&gt; boot -s verboseFrom the output, we can see that the virtual fibre channel adapter scans for visible LUN’s presented through the adapter and runs cfgmgr against them. In this instance, the cfgmgr process is hanging, hence causing the HMC LED 2700 code. This particular LPAR has LUN’s presented to it from EMC storage through VIOS via NPIV, some of which had a state of NR (Not Ready).Note that I’ve deliberately masked the Symmetrix and LUN ID’s.Name: aix_lpar1 Symmetrix ID : XXXXXXXXXX Last updated at : Mon Mar 18 16:20:54 2013 Masking Views : Yes FAST Policy : Yes Devices (62): { --------------------------------------------------------- Sym Device Cap Dev Pdev Name Config Sts (MB) --------------------------------------------------------- XXXX N/A RDF1+TDEV NR 16386 XXXX N/A RDF1+TDEV RW 20481 XXXX N/A RDF1+TDEV RW 51203 XXXX N/A RDF1+TDEV RW 8633 XXXX N/A RDF1+TDEV NR 65537 XXXX N/A RDF1+TDEV RW 79873 XXXX N/A RDF1+TDEV RW 79873 XXXX N/A RDF1+TDEV NR 71681 XXXX N/A RDF1+TDEV NR 61440 XXXX N/A RDF1+TDEV NR 131074 XXXX N/A RDF1+TDEV NR 153600 XXXX N/A RDF1+TDEV NR 212993 XXXX N/A RDF1+TDEV NR 311303 XXXX N/A RDF1+TDEV NR 16385 XXXX N/A RDF1+TDEV NR 16385 XXXX N/A RDF1+TDEV NR 16385 XXXX N/A RDF1+TDEV NR 16385 XXXX N/A RDF1+TDEV NR 16385 ... ...The LUN’s masked to this host have a clone configured from another AIX LPAR, which at some point resulted in the target LUN’s (our HMC LED 2700 host) being placed in a NR state. Resyncing the target LUN’s from the source put them back into the correct state, and allowed the AIX LPAR to boot successfully. During investigation, a PMR was raised with IBM, which eventually came to the same conclusion that EMC LUN’s in an NR state will cause an AIX LPAR to hang during boot. The cfgmgr operation will eventually fail the LUNs, and the LPAR will boot, but in our instance, with so many LUN’s in an NR state, failures (timeouts) across all LUNs wouldn’t have happened for many hours.Preventing this from occuring in the future would be to verify from the storage end that the LUNs are in a ready state prior to boot, or to place all those LUNs in a ‘Defined’ state in AIX before shutting down. There wasn’t much information when I looked for HMC LED 2700 hangs, so hopefully this helps someone with an AIX environment and EMC storage having the same symptoms." }, { "title": "Manually fixing dirty JFS2 filesystems", "url": "/posts/Manually-fixing-dirty-JFS2-filesystems/", "categories": "aix", "tags": "aix, filesystem, fsdb, jfs2", "date": "2012-06-19 00:00:00 +1000", "snippet": "The problemI think at some point during a systems administrators life span, they see the below error message when trying to mount a filesystem.# mount /testReplaying log for /dev/fslv01.mount: /dev...", "content": "The problemI think at some point during a systems administrators life span, they see the below error message when trying to mount a filesystem.# mount /testReplaying log for /dev/fslv01.mount: /dev/fslv01 on /test: Unformatted or incompatible mediaThe superblock on /dev/fslv01 is dirty. Run a full fsck to fix.Great! Let’s run the fsck command, cross our fingers, and hope that the superblock is repaired and we can mount the filesystem. However, say the dirty filesystem is 3 TB in size? Depending on the extent of the damage, running an fsck on a 3 TB filesystem can take quite some time (we’re talking hours here!). Let’s also paint a perfect picture and say that we’re willing to let fsck run its course no matter how long it takes (This is the preffered method). What if though, the fsck fails, and we still can’t mount the filesystem?The optionsWe have two options here: Restore the filesystem from backup Use the fsdb command to edit the superblock and mark the filesystem as clean.Taken from the fsdb man page: “The fsdb command enables you to examine, alter, and debug a file system, specified by the FileSystem parameter. The command provides access to file system objects, such as blocks, i-nodes, or directories. You can use the fsdb command to examine and patch damaged file systems. Key components of a file system can be referenced symbolically. This feature simplifies the procedures for correcting control-block entries and for descending the file system tree.” At this stage, I’d like to point out that manually editing filesystem objects is dangerous. It’s imperitive that you have a backup of the filesystem data, which in a worst case scenario, you can restore from.Using fsdbThe below is done on a test 10 GB JFS2 filesystem. After the example I explain a little more on exactly what we’re modifying.# fsdb /test File System: /test File System Size: 20970472 (512 byte blocks)Aggregate Block Size: 4096Allocation Group Size: 32768 (aggregate blocks) &gt; su[1] s_magic: 'J2FS' [18] s_fscklog: 1[2] s_version: 2 [19] s_fsckloglen: 50[3] s_size: 0x00000000013ffbe8 [20] s_bsize: 4096[4] s_logdev: 0x8000000a00000003 [21] s_logserial: 0x0000000a[5] s_l2bsize: 12 [22] s_logpxd.len: 0[6] s_l2bfactor: 3 [23] s_logpxd.addr1: 0x00[7] s_pbsize: 512 [24] s_logpxd.addr2: 0x00000000[8] s_l2pbsize: 9 s_logpxd.address: 0[9] s_rsv: Not Displayed [25] s_fsckpxd.len: 131[10] s_agsize: 0x00008000 [26] s_fsckpxd.addr1: 0x00[11] s_flag: 0x00000100 [27] s_fsckpxd.addr2: 0x0027ff7d s_fsckpxd.address: 2621309 [28] s_ait.len: 4 J2_GROUPCOMMIT [29] s_ait.addr1: 0x00 [30] s_ait.addr2: 0x0000000b s_ait.address: 11[12] s_state: 0x00000002 [31] s_fpack: 'fslv01' FM_DIRTY [32] s_fname: ''[13] s_time.tj_sec: 0x000000004fdfdce1 [33] s_time.tj_nsec: 0x00000000[14] s_ait2.len: 4 [34] s_xfsckpxd.len: 0[15] s_ait2.addr1: 0x00 [35] s_xfsckpxd.addr1: 0x00[16] s_ait2.addr2: 0x00000155 [36] s_xfsckpxd.addr2: 0x00000000 s_ait2.address: 341 s_xfsckpxd.address: 0[17] s_xsize: 0x0000000000000000 [37] s_xlogpxd.len: 0[40] feature_compat: 0x0000000000000001 [38] s_xlogpxd.addr1: 0x00[41] feature_rdonly: 0x0000000000000000 [39] s_xlogpxd.addr2: 0x00000000[42] feature_incompat: 0x0000000000000000 s_xlogpxd.address: 0[43-49] &lt;...snapshot info...&gt; [50] s_maxext: 0x00000000display_super: [m]odify, [s]napshot info or e[x]it: mPlease enter: field-number value &gt; 12 0x0[1] s_magic: 'J2FS' [18] s_fscklog: 1[2] s_version: 2 [19] s_fsckloglen: 50[3] s_size: 0x00000000013ffbe8 [20] s_bsize: 4096[4] s_logdev: 0x8000000a00000003 [21] s_logserial: 0x0000000a[5] s_l2bsize: 12 [22] s_logpxd.len: 0[6] s_l2bfactor: 3 [23] s_logpxd.addr1: 0x00[7] s_pbsize: 512 [24] s_logpxd.addr2: 0x00000000[8] s_l2pbsize: 9 s_logpxd.address: 0[9] s_rsv: Not Displayed [25] s_fsckpxd.len: 131[10] s_agsize: 0x00008000 [26] s_fsckpxd.addr1: 0x00[11] s_flag: 0x00000100 [27] s_fsckpxd.addr2: 0x0027ff7d s_fsckpxd.address: 2621309 [28] s_ait.len: 4 J2_GROUPCOMMIT [29] s_ait.addr1: 0x00 [30] s_ait.addr2: 0x0000000b s_ait.address: 11[12] s_state: 0x00000000 [31] s_fpack: 'fslv01' FM_CLEAN [32] s_fname: ''[13] s_time.tj_sec: 0x000000004fdfdce1 [33] s_time.tj_nsec: 0x00000000[14] s_ait2.len: 4 [34] s_xfsckpxd.len: 0[15] s_ait2.addr1: 0x00 [35] s_xfsckpxd.addr1: 0x00[16] s_ait2.addr2: 0x00000155 [36] s_xfsckpxd.addr2: 0x00000000 s_ait2.address: 341 s_xfsckpxd.address: 0[17] s_xsize: 0x0000000000000000 [37] s_xlogpxd.len: 0[40] feature_compat: 0x0000000000000001 [38] s_xlogpxd.addr1: 0x00[41] feature_rdonly: 0x0000000000000000 [39] s_xlogpxd.addr2: 0x00000000[42] feature_incompat: 0x0000000000000000 s_xlogpxd.address: 0[43-49] &lt;...snapshot info...&gt; [50] s_maxext: 0x00000000display_super: [m]odify, [s]napshot info or e[x]it: x&gt; q# mount /test# df -g /testFilesystem GB blocks Free %Used Iused %Iused Mounted on/dev/fslv01 10.00 10.00 1% 4 1% /testAs you can see from the example, we’re now able to mount the /test filesystem. We did this by telling the filesystem that it was “clean”. A breakdown of the important lines follows. Line Description 1 Invokes fsdb on the /test filesystem 9 Shows the superblock 26 Show field number [12] s_state is marked as FM_DIRTY, represented by the value 0x2 38 Puts fsdb into modify mode 39 Changes field number [12] s_state to FM_CLEAN by changing the value to 0x0 In all cases, you’d want to run fsck to fix this issue. If fsck fails, restoring from a backup will at least ensure data integrity. I only show this as an alternative method for those that have run into a roadblock, don’t have a backup, and need to salvage as much data on the filesystem as possible. Once again, use this at your own risk!" }, { "title": "Automatically reduce image.data to a single PV", "url": "/posts/Automatically-reduce-image.data-to-a-single-PV/", "categories": "aix, powervm, shell-scripting", "tags": "aix, image-data, ksh, scripting, shell, shell-scripting, volume-group", "date": "2012-04-04 00:00:00 +1000", "snippet": "I’ve been working with a client who is going through the process of migrating from physical Power 5 servers to a virtualized Power 7 environment with PowerVM. Due to the I/O limitations in the P5 s...", "content": "I’ve been working with a client who is going through the process of migrating from physical Power 5 servers to a virtualized Power 7 environment with PowerVM. Due to the I/O limitations in the P5 servers, our only method of migration was to take mksysb/savevg’s of the current servers, create NIM resources out of them, and then restore onto the P7 LPAR’s.The Power 5 rootvg consisted of two internal disks in a LVM mirror, with the other volume groups backed by either internal disk, or locally attached storage. The Power 7 which we were migrating to had it’s storage provided by a shiny DS8800. Given the boot from SAN solution we had, we no longer required two disks to form the rootvg, as all the mirroring and redundancy was being handled by the SVC’s. To successfuly restore the Power 5 mksysb onto the new Power 7 LPAR, we needed to provide an alternate image.data file during installation which has been modified to break the mirror and reduce the volume group down to a single physical disk. You can do this manually, and I would have if there was a small number of hosts to migrate, but I was dealing with a rather large enviornment.The below script does the following : Modifies the source_disk_data stanza to a single entry. Modifies the VG_SOURCE_DISK_LIST stanza to the disk set by the -d paramater. Modifies the LV_SOURCE_DISK_LIST stanza to the disk set by the -d paramater. Modifies the COPIES stanza to 1. Modifies the PP stanza to match the LPs value.The script takes two values, the resulting hdisk on the other node (In my case, hdisk0 for rootvg, and hdisk1 for datavg), and the location of the image.data or vg.data file.root@AIX #&gt; ./modify_data.shUsage: ./modify_data.sh -d &lt;hdisk&gt; -f &lt;file&gt; -d Destination hdisk on P7 LPAR -f image.data or vg.data file to modify -h Script usage#!/bin/ksh## Name:#\tmodify_data.sh# Description:# This script is for use during migrations where mksysb/savevg # consists of multiple physical volumes and is being restored# to a single volume. ## This script modifies the image.data or vg.data file to allow# the restore to a single volume.## Usage:# # ./modify_data.sh -d &lt;hdisk&gt; -f &lt;file&gt;# -d Destination hdisk on P7 LPAR# -f image.data or vg.data file to modify# -h Script usage# #*********************************************************## Functions, Variables, Checks and Command line options *##*********************************************************# # VariablesDATE=`date +%Y%m%d`PWD=`pwd` # Usage function_usage() { echo \"Usage: $0 -d &lt;hdisk&gt; -f &lt;file&gt;\" echo \" -d Destination hdisk on P7 LPAR\" echo \" -f image.data or vg.data file to modify\" echo \" -h Script usage\" echo \"\" echo \"Example:\" echo \" $0 -d hdisk0 -f image.data\"} # Display usage if no command line argumentsif [ $# -eq 0 ]; then _usage exit 1fi # Read command line optionswhile getopts \"d:f:h\" OPTION do case ${OPTION} in d) HDISK=\"${OPTARG}\";; f) FILE=\"${OPTARG}\";; h|*) _usage;; esac done # Check if file existsif [ ! -f $FILE ]; then echo \"File \\\"${FILE}\\\" does not exist\" exit 2fi #*********## * Main *##*********# # Make a copy of the current filecp $FILE ${PWD}/${FILE}.${DATE} # Modify source_disk_data stanzaawk '/source_disk_data:/{c=8}!(c&amp;&amp;c--)' ${FILE} &gt; /tmp/${FILE}.tmp1 sed '/source_disk_data;/ a\\\\source_disk_data:\\ PVID= 000107b0e68c43ec\\ PHYSICAL_LOCATION= U787F.001.DQM0XW6-P1-C4-T1-L3-L0\\ CONNECTION= scsi0//3,0\\ LOCATION= 00-08-00-3,0\\ SIZE_MB= 70006\\ HDISKNAME= '\"$HDISK\"'' /tmp/${FILE}.tmp1 &gt; /tmp/${FILE}.tmp2 # Modify VG_SOURCE_DISK_LIST, LV_SOURCE_DISK_LIST and# COPIES stanzased -e '/VG_SOURCE_DISK_LIST/ c\\ VG_SOURCE_DISK_LIST= '\"$HDISK\"'' -e '/LV_SOURCE_DISK_LIST/ c\\ LV_SOURCE_DISK_LIST= '\"$HDISK\"'' -e '/COPIES/ c\\ COPIES= 1' /tmp/${FILE}.tmp2 &gt; /tmp/${FILE}.tmp3 # Modify PP stanzaawk 'BEGIN { FS=\"=\"; modified=0} /COPIES/,/BB_POLICY/ { if ($1 ~ /LPs$/) { saved = $2; } if($1 ~ /PP$/) { var = saved; } else { var=$2; } print $1\"=\"var; modified = 1}{ if (modified == 0) { print; } else { modified = 0; }}' /tmp/${FILE}.tmp3 &gt; /tmp/${FILE}.tmp4 # Copy modified file to $PWDcp /tmp/${FILE}.tmp4 ${PWD}/${FILE}.1PV # Cleanup temp filesrm /tmp/${FILE}.tmp1rm /tmp/${FILE}.tmp2rm /tmp/${FILE}.tmp3rm /tmp/${FILE}.tmp4 # Outputecho \"\"echo \"Backup of ${FILE} is located at ${PWD}/${FILE}.${DATE}\"echo \"\"echo \"Modified file is located at ${PWD}/${FILE}.1PV\"echo \"\" exit 0As you can see, the script is very ugly and messy, but it does the job. I’ve successfully used this to do a number of migrations. Feel free to use, abuse or modify it. If you do modify it though, please let me know, and I can incorporate the changes into mine :)" }, { "title": "NFS cross-mounts in PowerHA", "url": "/posts/NFS-cross-mounts-in-PowerHA-HACMP/", "categories": "aix, powerha", "tags": "aix, nfs, powerha", "date": "2011-07-26 00:00:00 +1000", "snippet": "Combining NFS with PowerHA we can achieve a HANFS (Highly Available Network File System). The basic concept behind this solution is that one node in the cluster mounts the resource locally, and off...", "content": "Combining NFS with PowerHA we can achieve a HANFS (Highly Available Network File System). The basic concept behind this solution is that one node in the cluster mounts the resource locally, and offers that as an exported resource via a serviceable IP. Another node in the cluster is then configured to take on the resource in the event of failure.If you’re following this, I’m taking the assumption that your cluster is already configured, you have a working IP network and have set up a shared volume group between the cluster nodes that will be handling the HANFS failover. Before we get started though, there are a few things which need to be installed/verified.PrerequisitesThe cluster.es.nfs.rte fileset needs to be installed so that PowerHA can work with HANFS.# lslpp -l cluster.es.nfs.rte Fileset Level State Description ----------------------------------------------------------------------------Path: /usr/lib/objrepos cluster.es.nfs.rte 5.5.0.1 COMMITTED ES NFS Support Path: /etc/objrepos cluster.es.nfs.rte 5.5.0.1 COMMITTED ES NFS SupportAlso, since we’re dealing with NFS, we need to make sure we have the portmapper daemon running.# lssrc -s portmapSubsystem Group PID Status portmap portmap 213160 activeTo have the portmapper daemon start during system boot, you’ll need to modify /etc/rc.tcpip and uncomment the start line below.# Start up Portmapperstart /usr/sbin/portmap \"$src_running\"HANFS layoutBelow is a table of what the HANFS layout looks like. Resource Group Application Server Service IP Label Shared Volume Group Shared Logical Volumes Shared Filesystems Mount Point RG_nfs app_nfsv4 serviceip-nfs haNFSvg hanfs_kris_lv /hanfs_home_kristian /home/kristian         hanfs_data_lv /hanfs_data /data         hanfs_nfsstable_lv /nfs_stable /nfs_stable We have a Resource Group (RG_nfs) which contains an Application Server (app_nfsv4) and a Service IP Label (serviceip-nfs). There is a Shared Volume Group (haNFSvg) which contains some Shared Logical Volumes &amp; Filesystems. The thing to take note here, is that we are only exporting two of the three filesystems that you see in the table. The /nfs_stable filesystem will be the NFSv4 stable storage path location. “Stable Storage is a file system space that is used to save the state information by the NFSv4 server. This is very crucial for maintaining NFSv4 client’s state information to facilitate smooth and transparent fallover/fallback/move of the Resource group from one node to other.”ConfigurationWith the above information in hand, we’re ready to start configuring HANFS. (All Nodes) Set the NFS domain The NFSv4 domain needs to be set on all cluster nodes which will be responsible for taking on the RG_nfs resource group in the event of a failure. # chnfsdom &lt;- This will show the current NFS domain # chnfsdom [new_domain] &lt;- To set a new NFS domain (All Nodes) Create mount points The mount points need to be created on all cluster nodes which will be responsible for taking on the RG_nfs resource group in the event of a failure. # mkdir -p /home/kristian # mkdir -p /data Take note that we’re only creating the mount points here, and not the logical volumes or filesystems. Create logical volumes and filesystems Now we create the following logical volumes: hanfs_kris_lv hanfs_data_lv hanfs_nfsstable_lv Create the logical volumes using the following path. # smit hacmp -&gt; System Management (C-SPOC) -&gt; HACMP Logical Volume Management -&gt; Shared Logical Volumes -&gt; Add a Shared Logical Volume Create the filesystems using the following path. # smit hacmp -&gt; System Management (C-SPOC) -&gt; HACMP Logical Volume Management -&gt; Shared File Systems -&gt; Enhanced Journaled File Systems -&gt; Add an Enhanced Journaled File System on a Previously Defined Logical Volume We should now see the 2 filesystems which we are exporting and the stable storage filesystem apart of the shared volume group # lsvg -l haNFSvg haNFSvg: LV NAME TYPE LPs PPs PVs LV STATE MOUNT POINT hanfs_kris_lv jfs2 107 107 1 open/syncd /hanfs_home_kristian hanfs_data_lv jfs2 32 32 1 open/syncd /hanfs_data hanfs_nfsstable_lv jfs2 75 75 1 open/syncd /nfs_stable Configure the Application Server Luckily for us, IBM have made this step rather easy and have provided start, stop and monitor scripts. The location of these scripts are below. /usr/es/sbin/cluster/apps/clas_nfsv4/start /usr/es/sbin/cluster/apps/clas_nfsv4/stop /usr/es/sbin/cluster/apps/clam_nfsv4/monitor Create an Application Server using the following path. # smit hacmp -&gt; Extended Configuration -&gt; Extended Resource Configuration -&gt; HACMP Extended Resources Configuration -&gt; Configure HACMP Applications Servers -&gt; Add an Application Server To configure Application Server monitoring, use the following path. # smit hacmp -&gt; Extended Configuration -&gt; Extended Resource Configuration -&gt; HACMP Extended Resources Configuration -&gt; Configure HACMP Application Monitoring -&gt; Configure Custom Application Monitors -&gt; Add a Custom Application Monitor Configure the Resource Group for HANFS The next step is to configure the RG_nfs resource group with the values needed for HANFS. Modify the resource group using the following path. # smit hacmp -&gt; Extended Configuration -&gt; Extended Resource Configuration -&gt; HACMP Extended Resource Group Configuration -&gt; Change/Show Resources and Attributes for a Resource Group Service IP Labels/Addresses [serviceip-nfs] Application Servers [app_nfsv4] Filesystems (empty is ALL for VGs specified) [/hanfs_home_kristian /hanfs_data /nfs_stable] Filesystems Consistency Check fsck Filesystems Recovery Method sequential Filesystems mounted before IP configured true Filesystems/Directories to Export (NFSv2/3) [] Filesystems/Directories to Export (NFSv4) [/hanfs_home_kristian /hanfs_data] Stable Storage Path (NFSv4) [/nfs_stable] Filesystems/Directories to NFS Mount [/home/kristian;/hanfs_home_kristian /data;/hanfs_data] Most vaules are self explainatory. We set “Filesystems mounted before IP configured” to true so we prevent access from clients before the filesystems are ready. We also specify mount points in the following format [mount point];[exported filesystem] HANFS exports file Just like NFS has /etc/exports, HANFS has /usr/es/sbin/cluster/etc/exports. If you need to specify NFS options, you MUST use /usr/es/sbin/cluster/etc/exports and not /etc/exports. For help creating the exports file, you can use smit mknfsexp. * Pathname of directory to export [] Anonymous UID [-2] Public filesystem? no * Export directory now, system restart or both both Pathname of alternate exports file [/usr/es/sbin/cluster/etc/exports] ... ... Synchronize the cluster We now need to synchronize our changes to the other cluster nodes # smit hacmp -&gt; Extended Configuration -&gt; Extended Verification and Synchronization Bring the Resource Group online We now bring the resource group online. It’s a good idea at this stage to tail the hacmp.out file to see any errors. To bring the resource group online. # smit hacmp -&gt; System Management (C-SPOC) -&gt; HACMP Resource Group and Application Management -&gt; Bring a Resource Group Online # tail -f /var/hacmp/log/hacmp.out ... +RG_nfs:cl_activate_nfs(.110):/home/kristian;/hanfs_home_kristian[nfs_mount+102] : Attempt 0/5 to NFS-mount at Jul 26 11:01:21.000 +RG_nfs:cl_activate_nfs(.110):/home/kristian;/hanfs_home_kristian[nfs_mount+103] mount -o vers=4,hard,intr serviceip-nfs:/hanfs_home_kristian /home/kristian You should now see the filesystems mounted. # mount node mounted mounted over vfs date options -------- -------------- --------------- ------ ------------ --------------- ... /dev/hanfs_kris_lv /hanfs_home_kristian jfs2 Jul 26 14:36 rw,log=INLINE /dev/hanfs_data_lv /hanfs_data jfs2 Jul 26 14:36 rw,log=INLINE /dev/hanfs_nfsstable_lv /nfs_stable jfs2 Jul 26 14:36 rw,log=INLINE serviceip-nfs /hanfs_home_kristian /home/kristian nfs4 Jul 26 14:36 vers=4,hard,intr serviceip-nfs /hanfs_data /data nfs4 Jul 26 14:36 vers=4,hard,intr What you’re seeing above is the output from the mount command ran from the cluster node which currently has the RG_nfs resource group ONLINE. You’ll notice that it has the shared logical volumes and filesystems mounted, then a local NFS export from itself to mount /home/kristian and /data. On the other cluster nodes, you will only see the NFS mounts. There you have it, NFS cross-mounts with PowerHA." } ]
